{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Lab 4: Binary classification.\n",
    "\n",
    "---\n",
    "\n",
    "## Will it rain tomorrow in Australia?\n",
    "\n",
    "In this lab, we will work with a dataset containing daily weather data from several weather stations in Australia. Based on weather conditions at a given day, we will try to predict if rain will occur the next day.\n",
    "\n"
   ],
   "id": "6f57d6cf5522aa31"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T11:38:49.072149Z",
     "start_time": "2024-09-05T11:38:48.383736Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('data/weatherAUS.csv')\n",
    "df.head()"
   ],
   "id": "a0b4a62a3c921c1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         Date Location  MinTemp  MaxTemp  Rainfall  Evaporation  Sunshine  \\\n",
       "0  2008-12-01   Albury     13.4     22.9       0.6          NaN       NaN   \n",
       "1  2008-12-02   Albury      7.4     25.1       0.0          NaN       NaN   \n",
       "2  2008-12-03   Albury     12.9     25.7       0.0          NaN       NaN   \n",
       "3  2008-12-04   Albury      9.2     28.0       0.0          NaN       NaN   \n",
       "4  2008-12-05   Albury     17.5     32.3       1.0          NaN       NaN   \n",
       "\n",
       "  WindGustDir  WindGustSpeed WindDir9am  ... Humidity9am  Humidity3pm  \\\n",
       "0           W           44.0          W  ...        71.0         22.0   \n",
       "1         WNW           44.0        NNW  ...        44.0         25.0   \n",
       "2         WSW           46.0          W  ...        38.0         30.0   \n",
       "3          NE           24.0         SE  ...        45.0         16.0   \n",
       "4           W           41.0        ENE  ...        82.0         33.0   \n",
       "\n",
       "   Pressure9am  Pressure3pm  Cloud9am  Cloud3pm  Temp9am  Temp3pm  RainToday  \\\n",
       "0       1007.7       1007.1       8.0       NaN     16.9     21.8         No   \n",
       "1       1010.6       1007.8       NaN       NaN     17.2     24.3         No   \n",
       "2       1007.6       1008.7       NaN       2.0     21.0     23.2         No   \n",
       "3       1017.6       1012.8       NaN       NaN     18.1     26.5         No   \n",
       "4       1010.8       1006.0       7.0       8.0     17.8     29.7         No   \n",
       "\n",
       "   RainTomorrow  \n",
       "0            No  \n",
       "1            No  \n",
       "2            No  \n",
       "3            No  \n",
       "4            No  \n",
       "\n",
       "[5 rows x 23 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Location</th>\n",
       "      <th>MinTemp</th>\n",
       "      <th>MaxTemp</th>\n",
       "      <th>Rainfall</th>\n",
       "      <th>Evaporation</th>\n",
       "      <th>Sunshine</th>\n",
       "      <th>WindGustDir</th>\n",
       "      <th>WindGustSpeed</th>\n",
       "      <th>WindDir9am</th>\n",
       "      <th>...</th>\n",
       "      <th>Humidity9am</th>\n",
       "      <th>Humidity3pm</th>\n",
       "      <th>Pressure9am</th>\n",
       "      <th>Pressure3pm</th>\n",
       "      <th>Cloud9am</th>\n",
       "      <th>Cloud3pm</th>\n",
       "      <th>Temp9am</th>\n",
       "      <th>Temp3pm</th>\n",
       "      <th>RainToday</th>\n",
       "      <th>RainTomorrow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008-12-01</td>\n",
       "      <td>Albury</td>\n",
       "      <td>13.4</td>\n",
       "      <td>22.9</td>\n",
       "      <td>0.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>W</td>\n",
       "      <td>44.0</td>\n",
       "      <td>W</td>\n",
       "      <td>...</td>\n",
       "      <td>71.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1007.7</td>\n",
       "      <td>1007.1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.9</td>\n",
       "      <td>21.8</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2008-12-02</td>\n",
       "      <td>Albury</td>\n",
       "      <td>7.4</td>\n",
       "      <td>25.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WNW</td>\n",
       "      <td>44.0</td>\n",
       "      <td>NNW</td>\n",
       "      <td>...</td>\n",
       "      <td>44.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1010.6</td>\n",
       "      <td>1007.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.2</td>\n",
       "      <td>24.3</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2008-12-03</td>\n",
       "      <td>Albury</td>\n",
       "      <td>12.9</td>\n",
       "      <td>25.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WSW</td>\n",
       "      <td>46.0</td>\n",
       "      <td>W</td>\n",
       "      <td>...</td>\n",
       "      <td>38.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1007.6</td>\n",
       "      <td>1008.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>23.2</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2008-12-04</td>\n",
       "      <td>Albury</td>\n",
       "      <td>9.2</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NE</td>\n",
       "      <td>24.0</td>\n",
       "      <td>SE</td>\n",
       "      <td>...</td>\n",
       "      <td>45.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1017.6</td>\n",
       "      <td>1012.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.1</td>\n",
       "      <td>26.5</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2008-12-05</td>\n",
       "      <td>Albury</td>\n",
       "      <td>17.5</td>\n",
       "      <td>32.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>W</td>\n",
       "      <td>41.0</td>\n",
       "      <td>ENE</td>\n",
       "      <td>...</td>\n",
       "      <td>82.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>1010.8</td>\n",
       "      <td>1006.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>29.7</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The dataset contains 23 columns. Some of them contain numerical data, and some of them are categorical. Please note the 'Date' column, which contains the date of the observation in yyyy-mm-dd format.\n",
    "\n",
    "The target column is 'RainTomorrow', which contains information if it will rain tomorrow. The other columns are features that we can use to predict the target.\n",
    "\n",
    "## Exercise 1: Data exploration and preprocessing (2 point)\n",
    "\n",
    "- Extract names of all columns in the dataset. Which columns are numerical, and which are categorical? Check for the number of unique values in each categorical column and print the results. \n",
    "- Check if there are any missing values in the dataset. Print the percentage of missing values for each column.\n",
    "- Draw distribution of the target column 'RainTomorrow' on a histogram. Is the dataset balanced? What is the problem with training a model on an imbalanced dataset?\n",
    "- Check the size of the dataset before and after removing all missing values and print the results. Is it really a good idea to remove all missing values from the dataset?"
   ],
   "id": "34cf7c7777249310"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T10:51:03.928871Z",
     "start_time": "2024-09-05T10:51:03.924575Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Your code goes here\n",
    "..."
   ],
   "id": "1252f733a9fa630",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Let's encode the 'RainTomorrow' and 'RainToday' columns, which contain two categories 'Yes' and 'No'. We will use an approach called **label encoding**. The idea is to assign a unique integer $(0,1,2,3,...)$ to each category.\n",
    "\n",
    "<center>\n",
    "<img src=\"imgs/label-encoding.png\" width=500>\n",
    "</center>\n",
    "\n",
    "For example, if the 'RainTomorrow' column contains 'Yes' and 'No', we can assign $0$ to 'No' and $1$ to 'Yes'. One simple way to do this is to create a dictionary with the mapping and use the `map` function from the `pandas` library."
   ],
   "id": "6e928808432679b3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T11:08:23.965200Z",
     "start_time": "2024-09-05T11:08:23.781228Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mapping = {'No': 0, 'Yes': 1}\n",
    "\n",
    "df['RainTomorrow'] = df['RainTomorrow'].map(mapping)\n",
    "df['RainToday'] = df['RainToday'].map(mapping)\n",
    "\n",
    "df.head() # check the results"
   ],
   "id": "486929a01318cbab",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         Date Location  MinTemp  MaxTemp  Rainfall  Evaporation  Sunshine  \\\n",
       "0  2008-12-01   Albury     13.4     22.9       0.6          NaN       NaN   \n",
       "1  2008-12-02   Albury      7.4     25.1       0.0          NaN       NaN   \n",
       "2  2008-12-03   Albury     12.9     25.7       0.0          NaN       NaN   \n",
       "3  2008-12-04   Albury      9.2     28.0       0.0          NaN       NaN   \n",
       "4  2008-12-05   Albury     17.5     32.3       1.0          NaN       NaN   \n",
       "\n",
       "  WindGustDir  WindGustSpeed WindDir9am  ... Humidity9am  Humidity3pm  \\\n",
       "0           W           44.0          W  ...        71.0         22.0   \n",
       "1         WNW           44.0        NNW  ...        44.0         25.0   \n",
       "2         WSW           46.0          W  ...        38.0         30.0   \n",
       "3          NE           24.0         SE  ...        45.0         16.0   \n",
       "4           W           41.0        ENE  ...        82.0         33.0   \n",
       "\n",
       "   Pressure9am  Pressure3pm  Cloud9am  Cloud3pm  Temp9am  Temp3pm  RainToday  \\\n",
       "0       1007.7       1007.1       8.0       NaN     16.9     21.8        NaN   \n",
       "1       1010.6       1007.8       NaN       NaN     17.2     24.3        NaN   \n",
       "2       1007.6       1008.7       NaN       2.0     21.0     23.2        NaN   \n",
       "3       1017.6       1012.8       NaN       NaN     18.1     26.5        NaN   \n",
       "4       1010.8       1006.0       7.0       8.0     17.8     29.7        NaN   \n",
       "\n",
       "   RainTomorrow  \n",
       "0           NaN  \n",
       "1           NaN  \n",
       "2           NaN  \n",
       "3           NaN  \n",
       "4           NaN  \n",
       "\n",
       "[5 rows x 23 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Location</th>\n",
       "      <th>MinTemp</th>\n",
       "      <th>MaxTemp</th>\n",
       "      <th>Rainfall</th>\n",
       "      <th>Evaporation</th>\n",
       "      <th>Sunshine</th>\n",
       "      <th>WindGustDir</th>\n",
       "      <th>WindGustSpeed</th>\n",
       "      <th>WindDir9am</th>\n",
       "      <th>...</th>\n",
       "      <th>Humidity9am</th>\n",
       "      <th>Humidity3pm</th>\n",
       "      <th>Pressure9am</th>\n",
       "      <th>Pressure3pm</th>\n",
       "      <th>Cloud9am</th>\n",
       "      <th>Cloud3pm</th>\n",
       "      <th>Temp9am</th>\n",
       "      <th>Temp3pm</th>\n",
       "      <th>RainToday</th>\n",
       "      <th>RainTomorrow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008-12-01</td>\n",
       "      <td>Albury</td>\n",
       "      <td>13.4</td>\n",
       "      <td>22.9</td>\n",
       "      <td>0.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>W</td>\n",
       "      <td>44.0</td>\n",
       "      <td>W</td>\n",
       "      <td>...</td>\n",
       "      <td>71.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1007.7</td>\n",
       "      <td>1007.1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.9</td>\n",
       "      <td>21.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2008-12-02</td>\n",
       "      <td>Albury</td>\n",
       "      <td>7.4</td>\n",
       "      <td>25.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WNW</td>\n",
       "      <td>44.0</td>\n",
       "      <td>NNW</td>\n",
       "      <td>...</td>\n",
       "      <td>44.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1010.6</td>\n",
       "      <td>1007.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.2</td>\n",
       "      <td>24.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2008-12-03</td>\n",
       "      <td>Albury</td>\n",
       "      <td>12.9</td>\n",
       "      <td>25.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WSW</td>\n",
       "      <td>46.0</td>\n",
       "      <td>W</td>\n",
       "      <td>...</td>\n",
       "      <td>38.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1007.6</td>\n",
       "      <td>1008.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>23.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2008-12-04</td>\n",
       "      <td>Albury</td>\n",
       "      <td>9.2</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NE</td>\n",
       "      <td>24.0</td>\n",
       "      <td>SE</td>\n",
       "      <td>...</td>\n",
       "      <td>45.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1017.6</td>\n",
       "      <td>1012.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.1</td>\n",
       "      <td>26.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2008-12-05</td>\n",
       "      <td>Albury</td>\n",
       "      <td>17.5</td>\n",
       "      <td>32.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>W</td>\n",
       "      <td>41.0</td>\n",
       "      <td>ENE</td>\n",
       "      <td>...</td>\n",
       "      <td>82.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>1010.8</td>\n",
       "      <td>1006.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>29.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**This approach is simple and works well for some models, but it has a drawback**. If we encoded the 'Location' column with the following strategy:\n",
    "\n",
    "```python\n",
    "mapping = {\"Albury\": 0, \n",
    "           \"Sydney\": 1, \n",
    "           \"Melbourne\": 2,\n",
    "           ...}\n",
    "\n",
    "df['Location'] = df['Location'].map(mapping)\n",
    "```\n",
    "\n",
    "The model might deduce that 'Melbourne' is somehow more similar to 'Sydney' than to 'Albury', as 2 is closer to 1 than to 0. It may also expect that there is some order in the locations, which is, obviously, not the case. Label encoding may be especially harmful for linear models, as we risk introducing an artificial order in the data.\n",
    "\n",
    "To avoid those problems, we can use **one-hot encoding** (OHE). In this approach, we create a new binary column for each category. \n",
    "\n",
    "<center>\n",
    "<img src=\"imgs/ohe-encoding.png\" width=500>\n",
    "</center>\n",
    "\n",
    "All columns are independent, and the model will not assume any order in the data. The drawback is that the number of dimensions in the data increases significantly, which may lead to some problems related to [the curse of dimensionality](https://www.nature.com/articles/s41592-018-0019-x).\n",
    "\n",
    "Although you may have an idea of how to implement one-hot encoding with some `pandas` and dataframe manipulations, OHE can be easily executed using the `get_dummies` function, as demonstrated below."
   ],
   "id": "5d29679634512f9f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T11:04:54.480227Z",
     "start_time": "2024-09-05T11:04:54.416402Z"
    }
   },
   "cell_type": "code",
   "source": [
    "locations_ohe = pd.get_dummies(df['Location'])\n",
    "locations_ohe.head() # check the results"
   ],
   "id": "e0fc1ebae5ecacf7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   Adelaide  Albany  Albury  AliceSprings  BadgerysCreek  Ballarat  Bendigo  \\\n",
       "0     False   False    True         False          False     False    False   \n",
       "1     False   False    True         False          False     False    False   \n",
       "2     False   False    True         False          False     False    False   \n",
       "3     False   False    True         False          False     False    False   \n",
       "4     False   False    True         False          False     False    False   \n",
       "\n",
       "   Brisbane  Cairns  Canberra  ...  Townsville  Tuggeranong  Uluru  \\\n",
       "0     False   False     False  ...       False        False  False   \n",
       "1     False   False     False  ...       False        False  False   \n",
       "2     False   False     False  ...       False        False  False   \n",
       "3     False   False     False  ...       False        False  False   \n",
       "4     False   False     False  ...       False        False  False   \n",
       "\n",
       "   WaggaWagga  Walpole  Watsonia  Williamtown  Witchcliffe  Wollongong  \\\n",
       "0       False    False     False        False        False       False   \n",
       "1       False    False     False        False        False       False   \n",
       "2       False    False     False        False        False       False   \n",
       "3       False    False     False        False        False       False   \n",
       "4       False    False     False        False        False       False   \n",
       "\n",
       "   Woomera  \n",
       "0    False  \n",
       "1    False  \n",
       "2    False  \n",
       "3    False  \n",
       "4    False  \n",
       "\n",
       "[5 rows x 49 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Adelaide</th>\n",
       "      <th>Albany</th>\n",
       "      <th>Albury</th>\n",
       "      <th>AliceSprings</th>\n",
       "      <th>BadgerysCreek</th>\n",
       "      <th>Ballarat</th>\n",
       "      <th>Bendigo</th>\n",
       "      <th>Brisbane</th>\n",
       "      <th>Cairns</th>\n",
       "      <th>Canberra</th>\n",
       "      <th>...</th>\n",
       "      <th>Townsville</th>\n",
       "      <th>Tuggeranong</th>\n",
       "      <th>Uluru</th>\n",
       "      <th>WaggaWagga</th>\n",
       "      <th>Walpole</th>\n",
       "      <th>Watsonia</th>\n",
       "      <th>Williamtown</th>\n",
       "      <th>Witchcliffe</th>\n",
       "      <th>Wollongong</th>\n",
       "      <th>Woomera</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 49 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### *Implement one-hot encoding yourself, with the use of the `pandas` library.\n",
    "\n",
    "If you are feeling confident with your `pandas` skills, you can try to implement one-hot encoding yourself.\n",
    "\n",
    "The function `ohe_encode` should take a single argument, `column`, which is a pandas Series containing categorical data. The function should return a dataframe with one-hot encoded data, where the columns are named after the categories present in the input."
   ],
   "id": "cf5b2954f7c9261d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def ohe_encode(column):\n",
    "    # Your code goes here\n",
    "    ..."
   ],
   "id": "4da9dc2d7f3d38c9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exercise 2: Encoding categorical features (1 point)\n",
    "\n",
    "- Encode 'Locations', 'WindGustDir', 'WindDir9am', 'WindDir3pm' columns using **one-hot encoding** strategy. Join the results with the original dataframe. Remember to drop the original columns containing non-encoded data from the resulting dataframe.What is the problem with training a model on an imbalanced dataset?"
   ],
   "id": "f0dd1c077c1bfe3b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T11:09:48.712370Z",
     "start_time": "2024-09-05T11:09:48.707901Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Your code goes here\n",
    "..."
   ],
   "id": "fb8f0ca7638d7a93",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exercise 3: Encoding the 'Date' column (1 point)\n",
    "\n",
    "The 'Date' column contains information about the date of the observation.\n",
    "\n",
    "- How many unique dates are present in the 'Date' column? Print the number of unique values.\n",
    "- **What would be the problem with encoding the 'Date' column using either label encoding or OHE?** Think about how to encode the 'Date' column in a meaningful way. There are many possible strategies, so choose the one that seems the most reasonable to you and encode the 'Date' column accordingly. Join the results with the original dataframe and remember to drop the original 'Date' column."
   ],
   "id": "f9f6a0220e6836de"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T12:09:30.134878Z",
     "start_time": "2024-09-05T12:09:30.130711Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Your code goes here\n",
    "..."
   ],
   "id": "e4433c11cc1d2f35",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "You are already familiar with the train-test split procedure. We will use it to split the dataset into training and test sets. We will also deal with missing values in the dataset.\n",
    "\n",
    "As you have seen, dropping all the rows with missing values reduces the size of our dataset by over 60%. This is unacceptable, and we will circumvent this problem by using imputation.\n",
    "\n",
    "Imputation is a process of replacing missing values with some estimated values. One of the simplest strategies is to replace missing values with the **mean** or the **median** of the column. You should be wary of using the mean, as it is sensitive to outliers. The median is more robust in this regard.\n",
    "\n",
    "In case of categorical data, you can replace missing values with the most frequent value in the column.\n",
    "\n",
    "**Remember that you should calculate the mean or median on the training set and use the same values to impute missing values in the test set**. This is crucial, as you should not assume that you have access to the test set during the training phase, which would be an obvious case of **data leakage**.\n",
    "\n",
    "## Exercise 4: Train-test split and fixing missing values (2 points)\n",
    "\n",
    "- Split the dataframe into X (features) and $y$ (target). The target column is 'RainTomorrow', and the features are all other columns.\n",
    "- Split the features $X$ and labels $y$ into training and testing sets. Use 20% of the data for testing.\n",
    "- Calculate the **median** of each numerical column in the training set. Use the median to impute missing values in the training set. For categorical columns, use the most frequent value in the column for imputation.\n",
    "- Apply the same imputation to the test set."
   ],
   "id": "d5841c67369333c6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T12:09:39.021379Z",
     "start_time": "2024-09-05T12:09:39.017362Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Your code goes here\n",
    "..."
   ],
   "id": "c2cf55a597a519eb",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Measuring performance of a classifier\n",
    "\n",
    "Before we go on to training your weather predictor, let's discuss how to measure the performance of a trained classifier.\n",
    "\n",
    "In binary classification, we have four possible outcomes for each sample:\n",
    "- **True positive (TP)**: The classifier correctly predicted the positive class.\n",
    "- **True negative (TN)**: The classifier correctly predicted the negative class.\n",
    "- **False positive (FP)**: The classifier incorrectly predicted the positive class.\n",
    "- **False negative (FN)**: The classifier incorrectly predicted the negative class.\n",
    "\n",
    "Those outcomes can be summarized in what is known as a **confusion matrix**.\n",
    "\n",
    "The two most common metrics for binary classification are **accuracy** and **ROC AUC**.\n",
    "\n",
    "**Accuracy** is the ratio of correctly predicted observations to the total observations. It is a simple metric, but it can be misleading when the dataset is imbalanced. For example, if 95% of the samples belong to class 0, a classifier that always predicts class 0 will achieve 95% accuracy. \n",
    "\n",
    "$$\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN} = \\frac{\\text{correct predictions}}{\\text{all predictions}}$$\n",
    "    \n",
    "**ROC AUC** is a more robust metric for imbalanced datasets. You can read about the math behind it [at this cool online ML course by Google](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc). ROC AUC score is the probability that the classifier will rank a randomly chosen positive sample higher than a randomly chosen negative sample.\n",
    "\n",
    "Accuracy and ROC AUC metrics are implemented in the `sklearn` library, and to import them, you can use the following code:\n",
    "    \n",
    "```python\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "```"
   ],
   "id": "93ad28b60985832d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Training a model\n",
    "\n",
    "If you have successfully completed the previous exercises, you should have a dataset without any missing values, split into training and test sets, with all the categorical columns encoded.\n",
    "\n",
    "Now you can train a model that predicts rain based on the weather conditions. There are many classifier models implemented in `sklearn` library that you can use. I suggest starting with the `LogisticRegression` model, and you can try other models later.\n",
    "\n",
    "## Exercise 5: Training a model (2 points)\n",
    "\n",
    "- Train a `LogisticRegression` model on the training set. Use the default parameters.\n",
    "- Calculate the accuracy of the model on the test set and print it.\n",
    "- Calculate ROC AUC score for the model on the test set and print it.\n",
    "- Try at least one other model from the `sklearn` library. Compare the results of both models. Which model performs better?\n",
    "\n",
    "    Among many classifier models implemented in `sklearn`, I suggest trying:\n",
    "    - `SVC`\n",
    "    - `RandomForestClassifier`\n",
    "    - `KNeighborsClassifier`"
   ],
   "id": "893447869d0072da"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "# Your code goes here\n",
    "..."
   ],
   "id": "112f0becd49b3f44"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Confusion matrix\n",
    "\n",
    "Accuracy and ROC AUC are useful metrics, but they do not provide detailed information about the classifier's performance. Let's take a look at the confusion matrix and derive two more important metrics: **precision** and **recall**.\n",
    "\n",
    "\n",
    "\n",
    "- **Precision** is a useful metric when the cost of producing a false positives is high. Imagine that your classifier model is trained to predict if a mushroom is edible or poisonous. The cost of making a mistake and classifying a poisonous mushroom as edible (**FP**) is very high, as we may happen to need a liver transplant in that case. With high precision we sacrifice some **TP** for the sake of avoiding **FP**.\n",
    "\n",
    "- **Recall** is a useful metric when we do not care about false positives too much, and just want to catch as many positive samples as possible. If your classifier is trained for some medical screening task, we want to catch as many sick patients as possible (**TP**), even if it means that some healthy patients will be classified as sick (**FP**). The cost of false positive is not high in this case, as the healthy patient will be correctly diagnosed later, in more rigorous tests."
   ],
   "id": "c48c7d5c9bbd79"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## *Class competition!\n",
    "Try to achieve the highest possible ROC AUC on the test set. You can experiment with different models, hyperparameters, and feature engineering techniques. Post your scores in an online leaderboard. The 3 students with the highest ROC AUC score will receive a cool sticker and will be encouraged to present their solutions during the next lab."
   ],
   "id": "cee4f6b2f0cf94f3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d54467fc23ea556a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
