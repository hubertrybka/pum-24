{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf0b608fc736e1d6",
   "metadata": {},
   "source": [
    "# Lab 2: Linear regression in NumPy\n",
    "\n",
    "Before starting this chapter, please go through the `numpy_intro.ipynb` notebook!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5479e346363e465c",
   "metadata": {},
   "source": [
    "## Exercise 1: Matrix operations in NumPy (3 points)\n",
    "\n",
    "Here are a bunch of linear algebra tasks you are supposed to complete with NumPy. Work with [NumPy documentation](https://numpy.org/doc/stable/reference/routines.linalg.html) (and your search engine of choice) to find the functionalities you need.\n",
    "\n",
    "1. Create a 3x3 matrix $A$.\n",
    "\n",
    "    $A = \\begin{bmatrix} 1 & 2 & -1 \\\\ 2 & 1 & 2 \\\\ -1 & 2 & 1 \\end{bmatrix}$\n",
    "    \n",
    "    Remeber: The matrix is created by passing a list of lists to `np.array`. The outer list represents the **rows** of the matrix, and the inner lists represent the **elements in each row**. The matrix is of shape (rows, columns).\n",
    "\n",
    "2. Calculate $A^T$ (the transpose of matrix $A$). Print the transpose.\n",
    "3. Calculate $A^{-1}$ (the inverse of matrix $A$). Print the inverse.\n",
    "4. Multiply matrix $A$ by its inverse. What do you get? Print the result."
   ]
  },
  {
   "cell_type": "code",
   "id": "5a97152a0e20ff17",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "\n",
    "# your code here\n",
    "..."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "173ba0e642709bb9",
   "metadata": {},
   "source": [
    "## Exercise 2: Rotating a vector (2 points)\n",
    "\n",
    "One of the first things you learn in a linear algebra class is how matrices can be used to rotate vectors in 2D space. It is also a common operation in computer graphics. To rotate a vector $v$ counterclokwise, you multiply it by a rotation matrix.\n",
    "\n",
    "$$v_{\\text{rotated}} = R v$$\n",
    "\n",
    "The rotation matrix for a 2D vector is given by:\n",
    "\n",
    "$R = \\begin{bmatrix} \\cos(\\theta) & -\\sin(\\theta) \\\\ \\sin(\\theta) & \\cos(\\theta) \\end{bmatrix}$\n",
    "\n",
    "where $\\theta$ is the angle of rotation in radians. You can convert an angle from degrees to radians using the formula $\\theta_{\\text{rad}} = \\theta_{\\text{deg}} \\times \\frac{\\pi}{180}$.\n",
    "\n",
    "1. Implement a function `rotate_vector(vector, angle)` that rotates a 2D vector (`np.array`) by a given angle in degrees. The function should return the rotated vector (as `np.array`).    "
   ]
  },
  {
   "cell_type": "code",
   "id": "2d9ad804be6c5cfa",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "from helpers.plotting import plot_vectors\n",
    "\n",
    "def rotate_vector(vector, angle):\n",
    "    # your code goes here\n",
    "    ...\n",
    "    \n",
    "    return rotated_vector"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "40837d6419a08161",
   "metadata": {},
   "source": [
    "# Test the function\n",
    "vector = np.array([1, 0])\n",
    "rotated_vector = rotate_vector(vector, 120)\n",
    "\n",
    "# Draw the original and rotated vectors (the plot_vectors function was already implemented by me, and it should work out of the box)\n",
    "plot_vectors(vector, rotated_vector)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d46e21bafb2f001b",
   "metadata": {},
   "source": [
    "## Linear regression info dump\n",
    "\n",
    "Linear regression is a simple machine learning model that tries to find a linear relationship between a dependent variable and one or more independent variables. The model is represented by the equation:\n",
    "\n",
    "$$y = w_0 + w_1x_1 + w_2x_2 + ... + w_nx_n$$\n",
    "\n",
    "where:\n",
    "- $y$ is the dependent variable\n",
    "- $w_0, w_1, w_2, ..., w_n$ are the model parameters (weights)\n",
    "- $x_1, x_2, ..., x_n$ are the independent variables\n",
    "- $w_0$ is the bias term\n",
    "\n",
    "You may be familiar with the equation in the form $y = ax + b$, where $a$ is the slope and $b$ is the y-intercept. This is a special case of linear regression with one independent variable, its weight $a = w_1$ and bias $b = w_0$. In general, linear regression can handle multiple independent variables, each with its own weight.\n",
    "\n",
    "**We can apply one simple trick to make the maths a bit more elegant.** Let us rewrite the equation as:\n",
    "\n",
    "$$y = w_0x_0 + w_1x_1 + w_2x_2 + ... + w_nx_n$$\n",
    "\n",
    "where $x_0$ is always equal to 1, thus $w_0x_0$ always equals $w_0$. This seems redundant, as **why would we even want to multiply the bias term by 1?** Turns out, by using this trick, we can write the equation in a more compact form based on the dot product of two vectors:\n",
    "\n",
    "$$y = x \\cdot w = x^Tw$$\n",
    "\n",
    "where:\n",
    "- $y$ is the dependent variable\n",
    "- $w$ is a column vector of weights (with $w_0$ as the bias term)\n",
    "- $x^T$ is a row vector of independent variables (with $x_0$ always equal to 1)\n",
    "\n",
    "<center>\n",
    "<img src=\"imgs/linear-regression.png\" width=400>\n",
    "</center>\n",
    "\n",
    "**With this approach, and thanks to the magic of matrix algebra, we can calculate the predicted values for all observations in the dataset at once**. Instead of $x^T$ being a row vector, we can treat $x$ as a matrix with each row representing an observation and each column representing a feature. This matrix is called the **design matrix** and is denoted by $X$. The weights vector $w$ stays the same, the math still works out.\n",
    "\n",
    "$$y = Xw$$\n",
    "\n",
    "**Here is how the design matrix is constructed for a dataset of $m$ observations and $n$ features:**\n",
    "\n",
    "$$X = \\begin{bmatrix} 1 & x_{11} & x_{12} & \\dots & x_{1n} \\\\ 1 & x_{21} & x_{22} & \\dots & x_{2n} \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ 1 & x_{m1} & x_{m2} & \\dots & x_{mn} \\end{bmatrix}$$\n",
    "\n",
    "### The normal equation\n",
    "\n",
    "The goal of linear regression is to find the weights that minimize the difference between the predicted values and the actual values of the dependent variable $y$. This difference is called the **loss** or **cost** of the model. The most common loss function used in linear regression is the **mean squared error** (MSE), which is the average of the squared differences between the predicted and actual values.\n",
    "\n",
    "**The normal equation** lets us calculate the vector of weights that minimizes the MSE loss function. We derive it by differentiating the loss function with respect to the weights and setting the result to zero. The normal equation is given by the formula:\n",
    "\n",
    "$$w = (X^TX)^{-1}X^Ty$$\n",
    "\n",
    "where:\n",
    "- $w$ is the column vector of weights\n",
    "- $X$ is the design matrix, as explained above\n",
    "- $y$ is the column vector of dependent variables\n",
    "\n",
    "Remember that matrix multiplication is not commutative (the order of the matrices matters)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba360bd0c633886",
   "metadata": {},
   "source": [
    "## Exercise 3: Implement linear regression in NumPy (5 points)\n",
    "\n",
    "In this exercise, you will implement a simple linear regression model as a Python class. The class should have the following methods:\n",
    "\n",
    "- `__init__()`: This method should initialize the weights to `None`.\n",
    "- `fit(X, y)`: This method should update the weights and bias based on the data (using the normal equation).\n",
    "    * `X` should be a matrix of independent variables in the shape of (observations, features).\n",
    "    * `y` should be a column vector of dependent variables in the shape of (observations, 1).\n",
    "- `predict(X)`: This method should return the predicted values (as a column vector) based on the stored weights and data.\n",
    "\n",
    "Please remember, that in order to crate the **design matrix** $X$, you need to **add the column of ones** to the matrix of independent variables. This column corresponds to $x_0$, which is multiplied by $w_0$ (the bias term)."
   ]
  },
  {
   "cell_type": "code",
   "id": "571aaebcf9c6d741",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "\n",
    "class LinReg:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.weights = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Use get_design_matrix to add the column of ones to X\n",
    "        # then calculate the weights using the normal equation\n",
    "        # and store them in the self.weights attribute\n",
    "        self.weights = ...\n",
    "        \n",
    "    def predict(self, X):\n",
    "        ...\n",
    "        \n",
    "    def get_design_matrix(self, X):\n",
    "        ones = np.ones((X.shape[0], 1))\n",
    "        return np.hstack((ones, X))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ca58e5e72280929c",
   "metadata": {},
   "source": [
    "### *Fit a linear regression model to some data\n",
    "\n",
    "**Here is some synthetic data which you can use to test your model.** Launch the code below to see how your linear regression model finds a solution to the least squares problem."
   ]
  },
  {
   "cell_type": "code",
   "id": "79303dba805b99dd",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# load some synthetic data\n",
    "df = pd.read_csv('data/linear-regression.csv')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a88cbf4659d05441",
   "metadata": {},
   "source": [
    "# visualize the data\n",
    "sns.scatterplot(data=df, x='X', y='y')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2125c68376a31907",
   "metadata": {},
   "source": [
    "# split the data into X and y\n",
    "X = np.array(df['X']).reshape(-1, 1)    # reshape X to (n observations x 1 feature)\n",
    "y = np.array(df['y']).reshape(-1, 1)    # reshape y to (n observations x 1 target)\n",
    "\n",
    "model = LinReg()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b8a66fa14eb3703a",
   "metadata": {},
   "source": [
    "# fit the model to the data\n",
    "\n",
    "model.fit(X, y)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "584768212f09642e",
   "metadata": {},
   "source": [
    "# see the trained parameters\n",
    "\n",
    "b, a = model.weights    # in reverse, because the first element in the weights vector is the bias (w0)\n",
    "print(\"Slope:\", a)\n",
    "print(\"Bias:\", b)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ab6e95bdc6f189cd",
   "metadata": {},
   "source": [
    "# plot the regression line (we will learn how to do such plots in the next lab)\n",
    "\n",
    "sns.scatterplot(data=df, x='X', y='y')\n",
    "\n",
    "xs = np.linspace(-3, 2, 100)\n",
    "ys = xs*a + b\n",
    "sns.lineplot(x=xs, y=ys, color='red')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "42b6818eb1323b9d",
   "metadata": {},
   "source": [
    "### Scikit-learn saves the trouble of implementing ML models by hand\n",
    "\n",
    "You do not usually implement linear regression from scratch in a real-world project. Instead, you would use a library such as `sklearn`, which provides a simple interface to train and use tons of different machine learning models. \n",
    "\n",
    "**The scikit-learn interface is very similar to the one you implemented in the previous exercise.** Each model is implemented as a class with the methods `fit` and `predict`. Besides, scikit-learn provides many useful tools for data preprocessing, model selection, and evaluation. We will work with scikit-learn models from now on!"
   ]
  },
  {
   "cell_type": "code",
   "id": "65bdbe3d6d4706e3",
   "metadata": {},
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "y = model.predict(X)\n",
    "\n",
    "print(\"Slope:\", model.coef_[0])\n",
    "print(\"Bias:\", model.intercept_)\n",
    "\n",
    "results_df = pd.DataFrame({'X': X.flatten(), 'y': y.flatten()})\n",
    "\n",
    "sns.scatterplot(data=df, x='X', y='y')\n",
    "sns.lineplot(data=results_df, x='X', y='y', color='red')"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
